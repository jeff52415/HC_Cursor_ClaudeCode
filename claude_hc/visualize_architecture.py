"""
Visualize Hyper-Connections Architecture

This script creates ASCII diagrams to help understand how hyper-connections work.
"""


def print_standard_residual():
    """Visualize standard residual connection"""
    print("="*80)
    print("STANDARD RESIDUAL CONNECTION (Pre-Norm)")
    print("="*80)
    print()
    print("     Input: h ∈ R^d")
    print("        │")
    print("        ├──────────────┐")
    print("        │              │")
    print("        │         [ LayerNorm ]")
    print("        │              │")
    print("        │         [ Attention/FFN ]")
    print("        │              │")
    print("        │         [ Dropout ]")
    print("        │              │")
    print("        └──────( + )───┘")
    print("               │")
    print("          Output: h ∈ R^d")
    print()
    print("Single hidden vector throughout")
    print()


def print_hyper_connections():
    """Visualize hyper-connections"""
    print("="*80)
    print("HYPER-CONNECTIONS (n=4)")
    print("="*80)
    print()
    print("     Input: H = [h₁, h₂, h₃, h₄] ∈ R^(4×d)")
    print("              │")
    print("              │")
    print("        ┌─────┴─────┐")
    print("        │  WIDTH    │  Compute: mix_h = α^T @ H")
    print("        │CONNECTION │  - α ∈ R^(4×5) (learnable)")
    print("        └─────┬─────┘  - Mixes 4 hidden vectors")
    print("              │")
    print("         mix_h[0]  (first mixed vector)")
    print("              │")
    print("        [ LayerNorm ]")
    print("              │")
    print("        [ Layer: Attention/FFN ]")
    print("              │")
    print("         layer_output ∈ R^d")
    print("              │")
    print("        ┌─────┴─────┐")
    print("        │  DEPTH    │  Compute: H_new = β * layer_output + mix_h[1:5]")
    print("        │CONNECTION │  - β ∈ R^4 (learnable)")
    print("        └─────┬─────┘  - Combines output with residuals")
    print("              │")
    print("     Output: H_new = [h₁', h₂', h₃', h₄'] ∈ R^(4×d)")
    print()
    print("Multiple hidden vectors maintained in parallel")
    print()


def print_final_pooling():
    """Visualize final pooling"""
    print("="*80)
    print("FINAL POOLING")
    print("="*80)
    print()
    print("     After L layers:")
    print()
    print("     H^L = [h₁^L, h₂^L, h₃^L, h₄^L] ∈ R^(4×d)")
    print("       │    │    │    │")
    print("       └────┼────┼────┘")
    print("            │    │")
    print("            └────┘")
    print("              │")
    print("         [ Sum rows ]")
    print("              │")
    print("          h^L = h₁^L + h₂^L + h₃^L + h₄^L")
    print("              │")
    print("        [ LayerNorm ]")
    print("              │")
    print("        [ Output Projection ]")
    print("              │")
    print("          logits ∈ R^vocab_size")
    print()


def print_width_connection_detail():
    """Detailed view of width connection"""
    print("="*80)
    print("WIDTH CONNECTION (Detailed)")
    print("="*80)
    print()
    print("Matrix α ∈ R^(4×5):")
    print()
    print("        ┌                    ┐")
    print("        │ α₁,₀  α₁,₁  α₁,₂  α₁,₃  α₁,₄ │")
    print("    α = │ α₂,₀  α₂,₁  α₂,₂  α₂,₃  α₂,₄ │")
    print("        │ α₃,₀  α₃,₁  α₃,₂  α₃,₃  α₃,₄ │")
    print("        │ α₄,₀  α₄,₁  α₄,₂  α₄,₃  α₄,₄ │")
    print("        └                    ┘")
    print()
    print("Input H ∈ R^(4×d):        ")
    print("                          ")
    print("        ┌      ┐          ")
    print("        │  h₁  │          ")
    print("    H = │  h₂  │          ")
    print("        │  h₃  │          ")
    print("        │  h₄  │          ")
    print("        └      ┘          ")
    print()
    print("Operation: mix_h = α^T @ H")
    print()
    print("Result mix_h ∈ R^(5×d):")
    print()
    print("            ┌                                                    ┐")
    print("            │ α₁,₀h₁ + α₂,₀h₂ + α₃,₀h₃ + α₄,₀h₄  (layer input)  │")
    print("            │ α₁,₁h₁ + α₂,₁h₂ + α₃,₁h₃ + α₄,₁h₄  (residual 1)   │")
    print("  mix_h =   │ α₁,₂h₁ + α₂,₂h₂ + α₃,₂h₃ + α₄,₂h₄  (residual 2)   │")
    print("            │ α₁,₃h₁ + α₂,₃h₂ + α₃,₃h₃ + α₄,₃h₄  (residual 3)   │")
    print("            │ α₁,₄h₁ + α₂,₄h₂ + α₃,₄h₃ + α₄,₄h₄  (residual 4)   │")
    print("            └                                                    ┘")
    print()
    print("First row: mix_h[0] goes to layer")
    print("Rows 1-4: mix_h[1:5] used in depth connection")
    print()


def print_depth_connection_detail():
    """Detailed view of depth connection"""
    print("="*80)
    print("DEPTH CONNECTION (Detailed)")
    print("="*80)
    print()
    print("Vector β ∈ R^4:  β = [β₁, β₂, β₃, β₄]")
    print()
    print("layer_output ∈ R^d  (from attention/FFN)")
    print()
    print("mix_h[1:5] ∈ R^(4×d)  (residual connections)")
    print()
    print("Operation: H_new = β * layer_output + mix_h[1:5]")
    print()
    print("Result H_new ∈ R^(4×d):")
    print()
    print("            ┌                                      ┐")
    print("            │ β₁ * layer_output + mix_h[1]  (h₁') │")
    print("  H_new =   │ β₂ * layer_output + mix_h[2]  (h₂') │")
    print("            │ β₃ * layer_output + mix_h[3]  (h₃') │")
    print("            │ β₄ * layer_output + mix_h[4]  (h₄') │")
    print("            └                                      ┘")
    print()
    print("Each hidden vector gets:")
    print("  - Weighted layer output (via β)")
    print("  - Residual connection (from mix_h)")
    print()


def print_dynamic_vs_static():
    """Compare static and dynamic HC"""
    print("="*80)
    print("STATIC vs DYNAMIC HYPER-CONNECTIONS")
    print("="*80)
    print()
    print("STATIC HC (SHC):")
    print("  α and β are fixed learnable parameters")
    print("  α ∈ R^(4×5), β ∈ R^4")
    print("  Same for all tokens in all sequences")
    print()
    print("DYNAMIC HC (DHC):")
    print("  α and β depend on input H")
    print()
    print("  α(H) = s_α ⊙ tanh(H̄W_α) + α_static")
    print("  β(H) = s_β ⊙ tanh(H̄W_β) + β_static")
    print()
    print("  Where:")
    print("    H̄ = LayerNorm(H)")
    print("    W_α ∈ R^(d×5), W_β ∈ R^d  (learnable)")
    print("    s_α, s_β ≈ 0.01  (small scaling factors)")
    print()
    print("  Different for each token and adapts to input")
    print()


def print_initialization():
    """Show initialization details"""
    print("="*80)
    print("INITIALIZATION (Equivalent to Pre-Norm)")
    print("="*80)
    print()
    print("For layer k with expansion rate n=4:")
    print()
    print("Initial α matrix (layer 0):")
    print()
    print("        ┌              ┐")
    print("        │ 1  1  0  0  0│  ← Am (selects h₁)")
    print("    α = │ 0  1  0  0  0│")
    print("        │ 0  0  1  0  0│  ← Ar (identity)")
    print("        │ 0  0  0  1  0│")
    print("        │ 0  0  0  0  1│")
    print("        └              ┘")
    print("           ↑     ↑")
    print("           Am    Ar")
    print()
    print("Initial β vector:")
    print()
    print("    β = [1, 1, 1, 1]  (all ones)")
    print()
    print("This gives:")
    print("  mix_h[0] = h₁  (first vector selected)")
    print("  H_new[i] = 1 * layer_output + h_i")
    print()
    print("Which is equivalent to:")
    print("  h = h + layer(norm(h))")
    print()
    print("Standard Pre-Norm residual connection!")
    print()


def print_sequential_parallel_duality():
    """Show how HC can be sequential or parallel"""
    print("="*80)
    print("SEQUENTIAL-PARALLEL DUALITY")
    print("="*80)
    print()
    print("HC can learn to arrange layers sequentially or in parallel")
    print()
    print("SEQUENTIAL (standard residual):")
    print()
    print("    α = ┌        ┐    β = [1, 1]")
    print("        │ 0  1  1│")
    print("        │ 1  1  0│")
    print("        │ 0  0  1│")
    print("        └        ┘")
    print()
    print("    Layer 1 → Layer 2 → ... (sequential)")
    print()
    print()
    print("PARALLEL (parallel transformer blocks):")
    print()
    print("  Odd layers:")
    print("    α = ┌        ┐    β = [1, 0]")
    print("        │ 0  1  0│")
    print("        │ 1  1  1│")
    print("        │ 1  1  1│")
    print("        └        ┘")
    print()
    print("  Even layers:")
    print("    α = ┌        ┐    β = [0, 1]")
    print("        │ 0  0  1│")
    print("        │ 0  1  0│")
    print("        │ 1  0  1│")
    print("        └        ┘")
    print()
    print("    ┌─ Layer 1 ─┐")
    print("    │           │")
    print("    ├─ Layer 2 ─┤  (parallel)")
    print("    │           │")
    print("    └───────────┘")
    print()
    print("HC learns soft mixtures between these extremes!")
    print()


def main():
    """Print all visualizations"""
    print()
    print("#"*80)
    print("#" + " "*30 + "HYPER-CONNECTIONS" + " "*31 + "#")
    print("#" + " "*26 + "Architecture Visualization" + " "*27 + "#")
    print("#"*80)
    print()

    print_standard_residual()
    input("Press Enter to continue...")

    print_hyper_connections()
    input("Press Enter to continue...")

    print_final_pooling()
    input("Press Enter to continue...")

    print_width_connection_detail()
    input("Press Enter to continue...")

    print_depth_connection_detail()
    input("Press Enter to continue...")

    print_dynamic_vs_static()
    input("Press Enter to continue...")

    print_initialization()
    input("Press Enter to continue...")

    print_sequential_parallel_duality()

    print()
    print("#"*80)
    print("#" + " "*28 + "Visualization Complete" + " "*29 + "#")
    print("#"*80)
    print()


if __name__ == "__main__":
    main()
